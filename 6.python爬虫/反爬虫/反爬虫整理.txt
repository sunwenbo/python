反爬虫：
1、访问网站需要登录用户名和密码；
2、登录时带有验证码验证功能 当某一用户访问次数过多后，就自动让请求跳转到一个验证码页面，只有在输入正确的验证码之后才能继续访问网站；
3、通过设置web前端服务：例如nginx过滤访问的User-Agent判断如果是非浏览器访问返回403，阻挡此次访问；
4、设置Cookie过期时间，如从登录开始获取一个新的Cookie，10分钟后如果退出需要重新登录，获取一个新的Cookie；
5、查看访问的日志发现可疑的IP进行拉黑处理（同一时间进行大量的访问，非常频繁的请求数据）；
6、https证书授权；
7、 robots.txt 在爬虫中的指导作用君子协议
来一般来说搜索引擎爬取网站时都会，先读取下robots.txt文件，并依照里面所设定的规则去爬取网站（当然是指没用登录限制的页面）
