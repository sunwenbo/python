（1）IP 限制 

IP 限制是很常见的一种反爬虫的方式。服务端在一定时间内统计 IP 地址的访问 次数，当次数、频率达到一定阈值时返回错误码或者拒绝服务。这种方式比较直接 简单，但在 IPv4 资源越来越不足的情况下，很多用户共享一个 IP 出口，典型的如“长 城宽带”等共享型的 ISP。另外手机网络中的 IP 地址也是会经常变化的，如果对这 些 IP 地址进行阻断，则会将大量的正常用户阻止在外。 

对于大多数不需要登录就可以进行访问的网站，通常也只能使用 IP 地址进行限 制。比如“Freelancer 网站”，大量的公开数据可以被访问，但同一个 IP 地址的访问 是有一定的限制的。针对 IP 地址限制非常有效的方式是，使用大量的“高匿名”代 理资源。这些代理资源可以对源 IP 地址进行隐藏，从而让对方服务器看起来是多个 IP 地址进行访问。另一种限制方式是，根据业务需要，对国内、国外的 IP 地址进行 单独处理，进而对国外的高匿名代理进行阻断，例如使用海外的 IP 地址访问“天眼 查网站”则无法访问。 

（2）验证码 

验证码是一种非常常见的反爬虫方式。服务提供方在 IP 地址访问次数达到一定 数量后，可以返回验证码让用户进行验证。这种限制在不需要登录的网页界面比较 常见，它需要结合用户的 cookie 或者生成一个特殊标识对用户进行唯一性判断，以 防止同一个 IP 地址访问频率过高。验证码的存在形式非常多，有简单的数字验证码、 字母数字验证码、字符图形验证码，网站也可以用极验验证码等基于用户行为的验 证码。针对简单验证码，可以使用打码平台进行破解。这种平台通过脚本上传验证 的图片，由打码公司雇用的人工进行识别。针对极验验证等更复杂的验证码，可以尝试模拟用户的行为绕过去，但通常比较烦琐，难度较大。谷歌所用的验证码更为 复杂，通常是用户端结合云端进行手工打码，但会带来整体成本较高的问题。要想绕过这些验证码的限制，一种思路是在出现验证码之前放弃访问，更换 IP 地址。ADSL 拨号代理提供了这种可能性。ADSL 通过拨号的方式上网，需要输入 ADSL 账号和密码，每次拨号就更换一个 IP 地址。不同地域的 IP 地址分布在多个地 址段，如果 IP 地址都能使用，则意味着 IP 地址量级可达千万。如果我们将 ADSL 主机作为代理，每隔一段时间主机拨号一次（换一个 IP），这样可以有效防止 IP 地 址被封禁。这种情况下，IP 地址的有效时限通常很短，通常在 1 分钟以下。结合大 量的 ADSL 拨号代理可以达到并行获取大量数据的可能。如果网站使用了一些特殊 的唯一性的标识，则很容易被对方网站识别到，从而改进反爬虫策略，面对这种情 况，单独切换 IP 地址也会无效。遇到这种情况，必须要搞清楚标识的生成方式，进 而模拟真实用户的访问。 

（3）登录限制 

登录限制是一种更加有效的保护数据的方式。网站或者 APP 可以展示一些基础的数据，当需要访问比较重要或者更多的数据时则要求用户必须登录。例如，在天 眼查网站中，如果想要查看更多的信息，则必须用账号登录；“知乎”则是必须在登 录后才能看到更多的信息。登录后，结合用户的唯一标识，可以进行计数，当访问 频度、数量达到一定阈值后即可判断为爬虫行为，从而进行拦截。针对“登录限制” 的方法，可以使用大量的账号进行登录，但成本通常比较高。 

针对微信小程序，可以使用 wx.login()方法，这种方式不需要用户的介入，因而 不伤害用户的体验。小程序调用后会获取用户的唯一标识，后端可以根据这个唯一 标识进行反爬虫的判断。 

（4）数据伪装 

在网页上，我们可以监听流量，然后模拟用户的正常请求。mitmproxy 等工具可 以监听特定网址的访问（通常是 API 的地址），然后将需要的数据存储下来。基于 Chrome Headless 的工具也可以监听到流量并进行解析。在这种情况下，某些网站会 对数据进行一些伪装来增加复杂度。例如，在某网站上展示的价格为 945 元，在 DOM 树中是以 CSS 进行了一些伪装。要想得到正确的数值，必须对 CSS 的规则进行一些 计算才行，某网站上展示的价格如图 1-1 所示。 

640?wx_fmt=png

图 1-1  某网站上展示的价格 

该网站使用特殊的字体对数据进行了伪装。例如，3400，对应显示的是 1400， 如图 1-2 所示。如果能够找到所有的字体对应的关系，则可以逆向出正确的价格。 

某电影网站使用特殊的字符进行数据隐藏，这种不可见的字符会增加复杂度， 但还是可以通过对应的 UTF-8 字符集找到对应关系，从而得到正确的值，如图 1-3 所示。 

640?wx_fmt=png

图 1-2  3400 显示为 1400 

640?wx_fmt=png

图 1-3  网站用特殊字符进行伪装 

对于这种伪装，可以人工分析目标网站的前端代码，对 CSS、JavaScript 和字符进行分析，推导出计算公式。在这种情况下，使用爬虫必须要非常小心，因为很可 能目标网站进行改版后，规则已经发生了变化，抓取到的数据便会无效。在爬虫程序的维护上可以增加一些数据有效性的检查，通过自动化或者人工的方式进行检查。例如，针对机票数据可以检查价格是否在一个合理的区间范围内，如果超出，则认为规则已经变化。更为复杂的方案是可以借助 OCR 技术，对所需要的区域进行识别， 然后对比抓取到的结果。 

（5）参数签名 

设计良好的 API 通常都要对参数使用签名（sign）来驱避非法请求，常见于手机 APP。APP 通过加密算法对请求的参数进行运算，从而得到一个签名。这个签名通常 和时间戳相关，并且在请求中附加上时间戳。在请求的参数固定的情况下，能够在一小段时间内生效。当请求发送到服务端后，服务端对参数、时间戳进行验证，比较签名是否一致。如果不一致，则判断为非法请求。这样做的好处是，可以保护请 求，即便是被抓包，在很短时间内这个请求就会失效。获取 APP 端的加密算法一般较为困难，通常需要进行反编译才能获得加密算法。然而现阶段绝大多数 APP 已经 被加壳（典型的如 360 加固、爱加密等），要进行反编译都很困难。另一种保护措施 是，将加密算法放到原生代码中进行编译，通常这些代码是 C 或 C++代码。由于原生代码相对于 Java 代码更难进行逆向工程，所以这给反编译又带来了更多的麻烦。 

针对这种参数签名的方法，没有太好的途径能够来解决，在逆向反编译无果的 情况下，可以试着找寻有没有其他的入口，例如，HTML5、微信小程序等。如果它 们请求了相同的 API，则很有可能在源代码中包含了加密算法。幸运的是，基于 JavaScript 开发的应用非常容易逆向分析，能够很快地获取加密算法，从而绕过 APP 的保护机制。如果这些方法都不奏效，则可以考虑模拟用户操作应用，通过抓包的方式采集到流量中的信息。但这种方式效率较低，如果要发出多个并发的请求，往往需要多个设备同时进行。 

（6）隐藏验证 

更复杂的反爬虫的方式之一是，隐藏验证。例如，在网站的防护上，通过 JavaScript 请求一些特殊的网址，可以得到一些特定的令牌（token），这样每次请求时即可生成

不同的令牌。甚至有些网站会在不可见的图片加上一些特殊的请求参数，从而识别 是否是真正的浏览器用户。这种情况下，想直接获取 API 进行请求通常行不通或者 非常困难，只能通过 Chrome Headless 等工具模拟用户的行为，从而规避这种情况。

（7）阻止调试 

在分析某旅游网站时发现，一旦打开浏览器的控制台界面，就会无限触发浏览器的 debugger 指令。深入研究代码发现，该网站在一个名为 leonid-tq-jq-v3-min.js 中 给所有的构造函数都加上了 debugger 这个关键字，导致任何对象的生成都会触发调试器。这样做的目的是阻止意外的脚本或程序进行跟踪调试，从而保护代码。这种情况下，可以构建一个修改过的 js 文件，去掉 debugger 关键字，使用 mitmproxy 转发流量并拦截 leonid-tq-jq-v3-min.js，将改后的 js 文件返回给浏览器，从而绕过这个限制，某旅游网调试界面如图 1-4 所示。 